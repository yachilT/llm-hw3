{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a42c8e3",
   "metadata": {},
   "source": [
    "## 4.3 The Traditional NLP Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d3eff",
   "metadata": {},
   "source": [
    "### dspy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af978385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "lm = dspy.LM('xai/grok-3-mini', max_tokens=6000, temperature=0.1, top_p=0.9)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Traverse a directory and read html files - extract text from the html files\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "def read_html_files(dir_name, directory=\"../PragmatiCQA-sources\"):\n",
    "    texts = []\n",
    "    for filename in os.listdir(os.path.join(directory, dir_name)):\n",
    "        if filename.endswith(\".html\"):\n",
    "            with open(os.path.join(directory, dir_name, filename), 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'html.parser')\n",
    "                texts.append(soup.get_text())\n",
    "    return texts\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "# Perform retrieval on a specific topic: read html files for the corresponding folder, index\n",
    "def make_search(topic):\n",
    "    corpus = read_html_files(topic)\n",
    "    max_characters = 10000 \n",
    "    topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "    return dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve, brute_force_threshold=max_characters)\n",
    "\n",
    "# Make a RAG module with a given retriever.\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.respond = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "    def forward(self, topic, question, literal, pragmatic):\n",
    "        search = make_search(topic)\n",
    "        context = \" \".join(search(question).passages)\n",
    "        output = self.respond(context=context, question=question)\n",
    "        return dspy.Prediction(response=output['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48bff5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_conversations_set(filename, dataset_dir=\"../PragmatiCQA/data\"):\n",
    "    conversations = []\n",
    "    with open(os.path.join(dataset_dir, filename), 'r') as f:\n",
    "        for line in f:\n",
    "            conversations.append(json.loads(line))\n",
    "    return [dspy.Example(topic=d['community'],\n",
    "                        question=d['qas'][0]['q'], \n",
    "                        response=d['qas'][0]['a'],\n",
    "                        literal=[text['text'] for text in d['qas'][0]['a_meta']['literal_obj']],\n",
    "                        pragmatic=[text['text'] for text in d['qas'][0]['a_meta']['pragmatic_obj']]).with_inputs('question', 'topic', 'literal', 'pragmatic') for d in conversations]\n",
    "\n",
    "pcqa_test = get_conversations_set(\"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c08d72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'topic': 'The Legend of Zelda', 'question': 'What year did the Legend of Zelda come out?', 'literal': ['FDS release February 21, 1986\\n', 'The Legend of Zelda is the first installment of the Zelda series. ', ' It centers its plot around a boy named Link , who becomes the central protagonist throughout the series. '], 'pragmatic': ['It came out as early as 1986 for the Famicom in Japan, and was later released in the western world, including Europe and the US in 1987.']}) (input_keys={'question', 'literal', 'pragmatic', 'topic'})\n",
      "Prediction(\n",
      "    response='1986'\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.evaluate import SemanticF1, Evaluate\n",
    "metric = SemanticF1(decompositional=True)\n",
    "\n",
    "model = RAG()\n",
    "print(pcqa_test[0].inputs())\n",
    "pred = model(**pcqa_test[0].inputs())\n",
    "print(pred)\n",
    "\n",
    "metric(pcqa_test[0], pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18ba9890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.58 / 71 (13.5%):  33%|███▎      | 71/213 [37:23<1:06:17, 28.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/23 21:44:54 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 21:44:59 ERROR dspy.utils.parallelizer: Error for Example({'topic': 'Peanuts Comics', 'question': 'how old is snoopy?', 'response': 'I am not sure; he first appeared in his comic strip in 1950 and was a dog, not a puppy, but it is not clear exactly how old he was.His birthday celebration day changed from August 28 in the early days to August 10 by 1968. ', 'literal': [\"I don't know\"], 'pragmatic': ['irst appeared in the October 4, 1950 ', \"n 1951, Snoopy's birthday was celebrated on August 28 . However, in 1968, his birthday was celebrated on August 10 .\"]}) (input_keys={'question', 'literal', 'pragmatic', 'topic'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.58 / 71 (13.5%):  34%|███▍      | 72/213 [37:53<1:06:52, 28.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/23 21:45:26 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 21:45:30 ERROR dspy.utils.parallelizer: Error for Example({'topic': 'Peanuts Comics', 'question': 'What breed is Snoopy in the Peanuts comic strip?', 'response': \"Snoopy is a Beagle, he's charlie brown's best friend, He's very cool.\", 'literal': ['He is the pet beagle of Charlie Brown (his best friend) who cares for him. Snoopy is blessed with a rich, Walter Mitty-like fantasy life.'], 'pragmatic': ['He is the pet beagle of Charlie Brown (his best friend) who cares for him. Snoopy is blessed with a rich, Walter Mitty-like fantasy life.']}) (input_keys={'question', 'literal', 'pragmatic', 'topic'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.58 / 71 (13.5%):  34%|███▍      | 73/213 [38:24<1:08:32, 29.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/23 21:45:44 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 21:45:48 ERROR dspy.utils.parallelizer: Error for Example({'topic': 'Peanuts Comics', 'question': 'who is the main character? ', 'response': 'Snoopy is the main character, his owner is Charlie Brown though.', 'literal': [' Charlie Brown , Lucy (rarely), Linus'], 'pragmatic': [' Snoopy Show ( 2021 animat']}) (input_keys={'question', 'literal', 'pragmatic', 'topic'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.58 / 71 (13.5%):  35%|███▍      | 74/213 [38:42<1:00:05, 25.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/23 21:46:18 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 21:46:22 ERROR dspy.utils.parallelizer: Error for Example({'topic': 'Peanuts Comics', 'question': 'who wrote peanuts?', 'response': 'Charles Monroe Schulz wrote Peanuts with the initial cast featuring only Charlie Brown, Shermy, Patty, and a beagle named Snoopy.', 'literal': ['Charles Monroe Schulz'], 'pragmatic': ['The initial cast of Peanuts was small, featuring only Charlie Brown , Shermy , Patty (not to be confused with Peppermint Patty ), and a beagle, Snoopy .']}) (input_keys={'question', 'literal', 'pragmatic', 'topic'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.58 / 71 (13.5%):  35%|███▌      | 75/213 [39:16<1:05:04, 28.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/23 21:46:59 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/23 21:47:04 ERROR dspy.utils.parallelizer: Error for Example({'topic': 'Peanuts Comics', 'question': 'snoopy is part of peanuts no?', 'response': 'Yes, Peanuts is the strip and Snoopy first appeared in it, along with good Ol Charlie Brown in 1950.', 'literal': ['October 4, 1950 (comic strip)'], 'pragmatic': ['Joe Cool World War I Flying Ace Easter Beagle \"M']}) (input_keys={'question', 'literal', 'pragmatic', 'topic'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - {\"code\":\"Some resource has been exhausted\",\"error\":\"Your team bc29b82a-19d1-4c03-acf7-cd8b768453be has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.58 / 71 (13.5%): 100%|██████████| 213/213 [39:58<00:00, 11.26s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/23 21:47:04 WARNING dspy.utils.parallelizer: Execution cancelled due to errors or interruption.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Execution cancelled due to errors or interruption.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m metric = SemanticF1(decompositional=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m evaluator = Evaluate(devset=pcqa_test, metric=metric, num_threads=\u001b[32m1\u001b[39m, display_progress=\u001b[38;5;28;01mTrue\u001b[39;00m, display_table=\u001b[38;5;28;01mTrue\u001b[39;00m, provide_feedback=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMs/hw3/nlp-with-llms-2025-hw3/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m callbacks = _get_active_callbacks(instance)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n\u001b[32m    330\u001b[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMs/hw3/nlp-with-llms-2025-hw3/.venv/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:171\u001b[39m, in \u001b[36mEvaluate.__call__\u001b[39m\u001b[34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs, callback_metadata)\u001b[39m\n\u001b[32m    167\u001b[39m         program._suggest_failures += dspy.settings.get(\u001b[33m\"\u001b[39m\u001b[33msuggest_failures\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction, score\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m results = \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devset) == \u001b[38;5;28mlen\u001b[39m(results)\n\u001b[32m    174\u001b[39m results = [((dspy.Prediction(), \u001b[38;5;28mself\u001b[39m.failure_score) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMs/hw3/nlp-with-llms-2025-hw3/.venv/lib/python3.11/site-packages/dspy/utils/parallelizer.py:48\u001b[39m, in \u001b[36mParallelExecutor.execute\u001b[39m\u001b[34m(self, function, data)\u001b[39m\n\u001b[32m     46\u001b[39m tqdm.tqdm._instances.clear()\n\u001b[32m     47\u001b[39m wrapped = \u001b[38;5;28mself\u001b[39m._wrap_function(function)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMs/hw3/nlp-with-llms-2025-hw3/.venv/lib/python3.11/site-packages/dspy/utils/parallelizer.py:203\u001b[39m, in \u001b[36mParallelExecutor._execute_parallel\u001b[39m\u001b[34m(self, function, data)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cancel_jobs.is_set():\n\u001b[32m    202\u001b[39m     logger.warning(\u001b[33m\"\u001b[39m\u001b[33mExecution cancelled due to errors or interruption.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mExecution cancelled due to errors or interruption.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[31mException\u001b[39m: Execution cancelled due to errors or interruption."
     ]
    }
   ],
   "source": [
    "from dspy.evaluate import SemanticF1, Evaluate\n",
    "metric = SemanticF1(decompositional=True)\n",
    "\n",
    "evaluator = Evaluate(devset=pcqa_test, metric=metric, num_threads=1, display_progress=True, display_table=True, provide_feedback=True)\n",
    "\n",
    "evaluator(RAG())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-with-llms-2025-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
